Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech. In this study, we address a key aspect of language proficiency assessment -- syntactic complexity.  We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways. First, it is both {\it robust} and {\it reliable}, producing  automatic scores that agree well with human rating compared to the state-of-the-art.  Second, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view.
